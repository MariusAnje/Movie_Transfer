{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd004f3ce0738d928d74413a2b10d0d4c487f39bbf2ffd0e3f43a6ab028b956cd75",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(dataset, indices):\n",
    "    subset = []\n",
    "    for i in indices:\n",
    "        subset.append(dataset[i])\n",
    "    return subset\n",
    "\n",
    "def slash_train_val(dataset, percentage):\n",
    "    total = len(dataset)\n",
    "    val_data = np.random.choice(list(range(total)),int(total*percentage), replace=False).tolist()\n",
    "    train_data = list(set(range(total)) - set(val_data))\n",
    "    return val_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1276\n7234\n"
     ]
    }
   ],
   "source": [
    "# dataSetFile: 2D numpy array, [# of moives, # of attributes]\n",
    "# dataSetFile[0] --> 211 elements, [0:50] plot embeddings, [50:209] attributes, [209] IMDB rating, [210] Douban rating; Missing data = -1\n",
    "dataSetFile = np.load(\"npAttrEmbOvwDoubanR.npy\")\n",
    "\n",
    "isMethod1 = True\n",
    "\n",
    "if isMethod1:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209],dataSetFile[i,209:210]]), dataSetFile[i,210])) # data normalization\n",
    "else:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209]]), dataSetFile[i,210])) # data normalization\n",
    "\n",
    "# IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $\\cap$ Douban\n",
    "IMDBintersectDouban = np.load(\"IMDBIntersectDouban.npy\")\n",
    "dataset_intersect = get_subset(dataset, IMDBintersectDouban)\n",
    "\n",
    "# IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $-$ Douban\n",
    "IMDBDifferenceDouban = np.load(\"IMDBDifferenceDouban.npy\")\n",
    "dataset_difference = get_subset(dataset, IMDBDifferenceDouban)\n",
    "\n",
    "percentage = 0.15 # percentage of the evaluation set\n",
    "\n",
    "# slash the three datasets into training and evaluation set\n",
    "train_total, val_total = slash_train_val(dataset, percentage)\n",
    "train_intersect, val_intersect = slash_train_val(dataset_intersect, percentage)\n",
    "train_difference, val_difference = slash_train_val(dataset_difference, percentage)\n",
    "\n",
    "# sanity check\n",
    "print(len(train_total))\n",
    "print(len(val_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we use\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        exp = 128\n",
    "        emb1 = 10\n",
    "        emb2 = 10\n",
    "        self.fc11 = nn.Linear(50, emb1)\n",
    "        self.fc12 = nn.Linear(159, emb2)\n",
    "        self.fc2 = nn.Linear(emb1+emb2, exp)\n",
    "        self.fc3 = nn.Linear(exp, exp)\n",
    "        self.fc4 = nn.Linear(exp,10)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.tail = nn.Softmax(dim=1)\n",
    "        self.drop = torch.nn.Dropout(0.0)\n",
    "    def forward(self, x):\n",
    "        x1 = self.drop(self.relu(self.fc11(x[:,:50])))\n",
    "        x2 = self.drop(self.relu(self.fc12(x[:,50:])))\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        x = self.drop(self.relu(self.fc3(x)))\n",
    "        return (self.tail(self.fc4(x)) * (torch.Tensor(list(range(1,11))).to(x.device))).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we use\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(210, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 10)\n",
    "        self.drop = torch.nn.Dropout(0.0)\n",
    "        self.tail = torch.nn.Softmax(dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.relu(self.fc1(x)))\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        return (self.tail(self.fc3(x)) * (torch.Tensor(list(range(1,11))).to(x.device))).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing functions for pytorch\n",
    "def train(epochs):\n",
    "    loss_min = 10000\n",
    "    iter_loader = tqdm(range(epochs))\n",
    "    for _ in iter_loader:\n",
    "        running_loss = 0.\n",
    "        i = 0\n",
    "        for data, labels in train_loader:\n",
    "            # data = expand_data(data)\n",
    "            data, labels = data.float().to(device), labels.to(device)\n",
    "            # print(data[0], labels[0])\n",
    "            # break\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(output.size())\n",
    "            loss = criteria(output, labels.view(output.size()).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "        # print(running_loss/i)\n",
    "        scheduler.step()\n",
    "        val_loss = val()\n",
    "        iter_loader.set_description(f\"{running_loss/i:.4f}, {val_loss:.4f}, {loss_min:.4f}\")\n",
    "        if val_loss < loss_min:\n",
    "            loss_min = val_loss\n",
    "\n",
    "def val():\n",
    "    running_loss = 0.\n",
    "    i = 0\n",
    "    for data, labels in val_loader:\n",
    "        # data = expand_data(data)\n",
    "        data, labels = data.float().to(device), labels.to(device)\n",
    "        output = model(data)\n",
    "        loss = criteria(output, labels.view(output.size()).float())\n",
    "        running_loss += loss.item()\n",
    "        i+=1\n",
    "    return running_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc1fbefff73945f48f898349a76d106b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and validation loader definition\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, train_intersect), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, val_intersect), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=0.)\n",
    "# optimizer = optim.Adam(model.parameters(), 0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [40, 60, 80], 0.8)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=10, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.0001, verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "TS = torch.utils.data.Subset(dataset, train_data)\n",
    "VS = torch.utils.data.Subset(dataset, val_data)\n",
    "train_loader = torch.utils.data.DataLoader(TS, batch_size=len(TS), shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(VS, batch_size=len(VS), shuffle=False)\n",
    "for X,y in train_loader:\n",
    "    pass\n",
    "regr = svm.SVR(kernel=\"poly\", degree=10, gamma=\"scale\", tol=1e-4, verbose=True)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(287.7985, dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "for X,y in val_loader:\n",
    "    pass\n",
    "T = regr.predict(X).astype(\"float32\")\n",
    "criteria(torch.Tensor(T), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in dataset_intersect:\n",
    "    a = i[-1]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}