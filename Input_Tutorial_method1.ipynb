{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(dataset, indices, region):\n",
    "    subset = []\n",
    "    for i in indices:\n",
    "        subset.append((dataset[i][region[0]], dataset[i][region[1]]))\n",
    "    return subset\n",
    "\n",
    "def slash_train_val(dataset, percentage):\n",
    "    total = len(dataset)\n",
    "    val_data = np.random.choice(list(range(total)),int(total*percentage), replace=False).tolist()\n",
    "    train_data = list(set(range(total)) - set(val_data))\n",
    "    return val_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1276\n7234\n"
     ]
    }
   ],
   "source": [
    "# dataSetFile: 2D numpy array, [# of moives, # of attributes]\n",
    "# dataSetFile[0] --> 211 elements, [0:50] plot embeddings, [50:209] attributes, [209] IMDB rating, [210] Douban rating; Missing data = -1\n",
    "dataSetFile = np.load(\"npAttrEmbOvwDoubanR.npy\")\n",
    "\n",
    "isMethod1 = True\n",
    "if isMethod1:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209],dataSetFile[i,209:210]]), dataSetFile[i,210])) # data normalization\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $\\cap$ Douban\n",
    "    IMDBintersectDouban = np.load(\"IMDBIntersectDouban.npy\")\n",
    "    dataset_intersect = get_subset(dataset, IMDBintersectDouban, [0,1])\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $-$ Douban\n",
    "    IMDBDifferenceDouban = np.load(\"IMDBDifferenceDouban.npy\")\n",
    "    dataset_difference = get_subset(dataset, IMDBDifferenceDouban, [0,1])\n",
    "    \n",
    "    dummy_rating = 0.\n",
    "    for i in range(len(dataset_difference)):\n",
    "        a = 1*dataset_difference[i][0][209:210]\n",
    "                \n",
    "        y = list(dataset_difference[i])\n",
    "        y[1] = a\n",
    "        dataset_difference[i] = tuple(y)\n",
    "        dataset_difference[i][0][209:210] = dummy_rating\n",
    "        #print(dataset_difference[i])\n",
    "\n",
    "\n",
    "\n",
    "    percentage = 0.15 # percentage of the evaluation set\n",
    "\n",
    "    # slash the three datasets into training and evaluation set\n",
    "    train_total, val_total = slash_train_val(dataset, percentage)\n",
    "    train_intersect, val_intersect = slash_train_val(dataset_intersect, percentage)\n",
    "    train_difference, val_difference = slash_train_val(dataset_difference, percentage)\n",
    "else:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209]]), dataSetFile[i,209], dataSetFile[i,210])) # data normalization\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $\\cap$ Douban\n",
    "    IMDBintersectDouban = np.load(\"IMDBIntersectDouban.npy\")\n",
    "    dataset_intersect = get_subset(dataset, IMDBintersectDouban, [0,2])\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $-$ Douban\n",
    "    IMDBDifferenceDouban = np.load(\"IMDBDifferenceDouban.npy\")\n",
    "    dataset_difference = get_subset(dataset, IMDBDifferenceDouban, [0,1])\n",
    "\n",
    "    percentage = 0.15 # percentage of the evaluation set\n",
    "\n",
    "    # slash the three datasets into training and evaluation set\n",
    "    train_total, val_total = slash_train_val(dataset, percentage)\n",
    "    train_intersect, val_intersect = slash_train_val(dataset_intersect, percentage)\n",
    "    train_difference, val_difference = slash_train_val(dataset_difference, percentage)\n",
    "\n",
    "# sanity check\n",
    "print(len(train_total))\n",
    "print(len(val_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we use\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        exp = 128\n",
    "        emb1 = 10\n",
    "        emb2 = 20 \n",
    "        self.fc11 = torch.nn.Linear(50, emb1)\n",
    "        self.fc12 = torch.nn.Linear(159, emb2)\n",
    "        self.fc13 = torch.nn.Linear(1, 1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(emb1+emb2+1, exp)\n",
    "        self.fc3 = nn.Linear(exp, exp)\n",
    "        self.fc4 = nn.Linear(exp,10)\n",
    "        self.drop = torch.nn.Dropout(0.0)\n",
    "        self.tail = torch.nn.Softmax(dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.drop(self.relu(self.fc11(x[:,:50])))\n",
    "        x2 = self.drop(self.relu(self.fc12(x[:,50:209])))\n",
    "        x3 = self.drop(self.fc13(x[:,209:210]))\n",
    "\n",
    "        x = torch.cat([x1,x2,x3],dim=1)\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        x = self.drop(self.relu(self.fc3(x)))\n",
    "        return (self.tail(self.fc4(x)) * (torch.Tensor(list(range(1,11))).to(x.device))).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing functions for pytorch\n",
    "def train(epochs):\n",
    "    loss_min = 10000\n",
    "    iter_loader = tqdm(range(epochs))\n",
    "    for _ in iter_loader:\n",
    "        running_loss = 0.\n",
    "        i = 0\n",
    "        for data, labels in train_loader:\n",
    "            # data = expand_data(data)\n",
    "            data, labels = data.float().to(device), labels.to(device)\n",
    "            # print(data[0], labels[0])\n",
    "            # break\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(output.size())\n",
    "            loss = criteria(output, labels.view(output.size()).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "        # print(running_loss/i)\n",
    "        scheduler.step()\n",
    "        val_loss = val()\n",
    "        iter_loader.set_description(f\"{running_loss/i:.4f}, {val_loss:.4f}, {loss_min:.4f}\")\n",
    "        if val_loss < loss_min:\n",
    "            loss_min = val_loss\n",
    "\n",
    "def val():\n",
    "    running_loss = 0.\n",
    "    i = 0\n",
    "    for data, labels in val_loader:\n",
    "        # data = expand_data(data)\n",
    "        data, labels = data.float().to(device), labels.to(device)\n",
    "        output = model(data)\n",
    "        loss = criteria(output, labels.view(output.size()).float())\n",
    "        running_loss += loss.item()\n",
    "        i+=1\n",
    "    return running_loss/i"
   ]
  },
  {
   "source": [
    "## Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fce9cf3bc58a48af9089f2ff29604648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and validation loader definition\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, train_intersect), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, val_intersect), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=0.)\n",
    "# optimizer = optim.Adam(model.parameters(), 0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [40, 60, 80], 0.8)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(300)"
   ]
  },
  {
   "source": [
    "### Baseline Experiment Log\n",
    "\n",
    "1. 0.5759\n",
    "2. 0.5799\n",
    "3. 0.5621\n",
    "4. 0.5809\n",
    "5. 0.5668\n",
    "6. 0.5650\n",
    "7. 0.5800\n",
    "8. 0.5970\n",
    "9. 0.5812\n",
    "10. 0.5696"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Transfer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=80.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "170edfff0af34138bb82b0f2287a9a29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=400.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09d79c122e4049279a437982384f2782"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_difference, train_difference), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_difference, val_difference), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=0.)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(80)\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, train_intersect), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, val_intersect), batch_size=1024, shuffle=False)\n",
    "criteria = nn.MSELoss()\n",
    "train(400)"
   ]
  },
  {
   "source": [
    "### Transfer Experiment Log\n",
    "\n",
    "**Dummy 10**\n",
    "\n",
    "[ 0.7434, 0.9122, 0.5614, 0.6050, 0.7922]\n",
    "\n",
    "**Dummy 9**\n",
    "\n",
    "[ 0.7883, 0.6051, 0.7863, 0.7953, 0.7238, 0.6112]\n",
    "\n",
    "**Dummy 8**\n",
    "\n",
    "[ 0.6023, 0.5829, 0.6209, 0.5828, 0.5941]\n",
    "\n",
    "**Dummy 7**\n",
    "\n",
    "[ 0.5996, 0.6643, 0.6754, 0.6413, 0.6284]\n",
    "\n",
    "**Dummy 6**\n",
    "\n",
    "[ 0.7778, 0.8221, 0.5897, 0.5838, 0.8313]\n",
    "\n",
    "**Dummy 5**\n",
    "\n",
    "[ 0.5475, 0.6244, 0.5576, 0.5408, 0.5596]\n",
    "\n",
    "**Dummy 4**\n",
    "\n",
    "[ 0.5587, 0.5641, 0.5684, 0.5684, 0.5623]\n",
    "\n",
    "**Dummy 3**\n",
    "\n",
    "[ 0.5927, 0.6364, 0.6037, 0.8130, 0.9224]\n",
    "\n",
    "**Dummy 2**\n",
    "\n",
    "[ 0.6093, 0.5988, 0.5695, 0.6039, 0.6277]\n",
    "\n",
    "**Dummy 1**\n",
    "\n",
    "[ 0.7500, 0.5535, 0.5578, 0.5623, 0.5652]\n",
    "\n",
    "**Dummy 0**\n",
    "\n",
    "[ 0.5564, 0.5637, 0.5995, 0.5636, 0.6034]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SVR Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=10, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.0001, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "TS = torch.utils.data.Subset(dataset_intersect, train_intersect)\n",
    "VS = torch.utils.data.Subset(dataset_intersect, val_intersect)\n",
    "train_loader = torch.utils.data.DataLoader(TS, batch_size=len(TS), shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(VS, batch_size=len(VS), shuffle=False)\n",
    "for X,y in train_loader:\n",
    "    pass\n",
    "regr = svm.SVR(kernel=\"poly\", degree=10, gamma=\"scale\", tol=1e-4, verbose=True)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7782, dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "for X,y in val_loader:\n",
    "    pass\n",
    "T = regr.predict(X).astype(\"float32\")\n",
    "criteria(torch.Tensor(T), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd004f3ce0738d928d74413a2b10d0d4c487f39bbf2ffd0e3f43a6ab028b956cd75",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}