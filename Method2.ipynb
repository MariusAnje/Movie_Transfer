{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd004f3ce0738d928d74413a2b10d0d4c487f39bbf2ffd0e3f43a6ab028b956cd75",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(dataset, indices, region):\n",
    "    subset = []\n",
    "    for i in indices:\n",
    "        subset.append((dataset[i][region[0]], dataset[i][region[1]]))\n",
    "    return subset\n",
    "\n",
    "def slash_train_val(dataset, percentage):\n",
    "    total = len(dataset)\n",
    "    val_data = np.random.choice(list(range(total)),int(total*percentage), replace=False).tolist()\n",
    "    train_data = list(set(range(total)) - set(val_data))\n",
    "    return val_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1276\n7234\n"
     ]
    }
   ],
   "source": [
    "# dataSetFile: 2D numpy array, [# of moives, # of attributes]\n",
    "# dataSetFile[0] --> 211 elements, [0:50] plot embeddings, [50:209] attributes, [209] IMDB rating, [210] Douban rating; Missing data = -1\n",
    "dataSetFile = np.load(\"npAttrEmbOvwDoubanR.npy\")\n",
    "\n",
    "isMethod1 = True\n",
    "if isMethod1:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209],dataSetFile[i,209:210]]), dataSetFile[i,210])) # data normalization\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $\\cap$ Douban\n",
    "    IMDBintersectDouban = np.load(\"IMDBIntersectDouban.npy\")\n",
    "    dataset_intersect = get_subset(dataset, IMDBintersectDouban, [0,1])\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $-$ Douban\n",
    "    IMDBDifferenceDouban = np.load(\"IMDBDifferenceDouban.npy\")\n",
    "    dataset_difference = get_subset(dataset, IMDBDifferenceDouban, [0,1])\n",
    "\n",
    "    percentage = 0.15 # percentage of the evaluation set\n",
    "\n",
    "    # slash the three datasets into training and evaluation set\n",
    "    train_total, val_total = slash_train_val(dataset, percentage)\n",
    "    train_intersect, val_intersect = slash_train_val(dataset_intersect, percentage)\n",
    "    train_difference, val_difference = slash_train_val(dataset_difference, percentage)\n",
    "else:\n",
    "    dataset = []\n",
    "    for i in range(len(dataSetFile)):\n",
    "        dataset.append((np.concatenate([dataSetFile[i,0:50]/1000, dataSetFile[i,50:209]]), dataSetFile[i,209], dataSetFile[i,210])) # data normalization\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $\\cap$ Douban\n",
    "    IMDBintersectDouban = np.load(\"IMDBIntersectDouban.npy\")\n",
    "    dataset_intersect = get_subset(dataset, IMDBintersectDouban, [0,2])\n",
    "\n",
    "    # IMDBintersectDouban: 1D array, movie indices in dataSetFile that IMDB $-$ Douban\n",
    "    IMDBDifferenceDouban = np.load(\"IMDBDifferenceDouban.npy\")\n",
    "    dataset_difference = get_subset(dataset, IMDBDifferenceDouban, [0,1])\n",
    "\n",
    "    percentage = 0.15 # percentage of the evaluation set\n",
    "\n",
    "    # slash the three datasets into training and evaluation set\n",
    "    train_total, val_total = slash_train_val(dataset, percentage)\n",
    "    train_intersect, val_intersect = slash_train_val(dataset_intersect, percentage)\n",
    "    train_difference, val_difference = slash_train_val(dataset_difference, percentage)\n",
    "\n",
    "# sanity check\n",
    "print(len(train_total))\n",
    "print(len(val_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we use\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(210, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 10)\n",
    "        self.drop = torch.nn.Dropout(0.0)\n",
    "        self.tail = torch.nn.Softmax(dim=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.relu(self.fc1(x)))\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        return (self.tail(self.fc3(x)) * (torch.Tensor(list(range(1,11))).to(x.device))).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing functions for pytorch\n",
    "def train(epochs):\n",
    "    loss_min = 10000\n",
    "    iter_loader = tqdm(range(epochs))\n",
    "    for _ in iter_loader:\n",
    "        running_loss = 0.\n",
    "        i = 0\n",
    "        for data, labels in train_loader:\n",
    "            # data = expand_data(data)\n",
    "            data, labels = data.float().to(device), labels.to(device)\n",
    "            # print(data[0], labels[0])\n",
    "            # break\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(output.size())\n",
    "            loss = criteria(output, labels.view(output.size()).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "        # print(running_loss/i)\n",
    "        scheduler.step()\n",
    "        val_loss = val()\n",
    "        iter_loader.set_description(f\"{running_loss/i:.4f}, {val_loss:.4f}, {loss_min:.4f}\")\n",
    "        if val_loss < loss_min:\n",
    "            loss_min = val_loss\n",
    "\n",
    "def val():\n",
    "    running_loss = 0.\n",
    "    i = 0\n",
    "    for data, labels in val_loader:\n",
    "        # data = expand_data(data)\n",
    "        data, labels = data.float().to(device), labels.to(device)\n",
    "        output = model(data)\n",
    "        loss = criteria(output, labels.view(output.size()).float())\n",
    "        running_loss += loss.item()\n",
    "        i+=1\n",
    "    return running_loss/i"
   ]
  },
  {
   "source": [
    "## Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0476d7cc4ea4e3f9b3a90295e98471a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and validation loader definition\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, train_intersect), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, val_intersect), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=0.)\n",
    "# optimizer = optim.Adam(model.parameters(), 0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [40, 60, 80], 0.8)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(300)"
   ]
  },
  {
   "source": [
    "## Source"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a9951a5883f4a27877cdde68f5d9532"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_difference, train_difference), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_difference, val_difference), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.1, momentum=0.9, weight_decay=0.)\n",
    "# optimizer = optim.Adam(model.parameters(), 0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [40, 60, 80], 0.8)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f642f859eeef4e23b683b0bc5d4ce865"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, train_intersect), batch_size=1024, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_intersect, val_intersect), batch_size=1024, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.001, momentum=0.9, weight_decay=0.)\n",
    "# optimizer = optim.Adam(model.parameters(), 0.1)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [40, 60, 80], 0.8)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [300, 600, 900], 0.1)\n",
    "criteria = nn.MSELoss()\n",
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_list = []\n",
    "for i in dataset_intersect:\n",
    "    a = i[-1]\n",
    "    the_list.append(a)\n",
    "the_list = np.array(the_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8157777351682093\n"
     ]
    }
   ],
   "source": [
    "the_mean = np.mean(the_list)\n",
    "print(((the_list - the_mean)**2/len(the_list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}